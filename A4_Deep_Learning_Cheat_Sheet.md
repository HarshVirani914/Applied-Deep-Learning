# ğŸ“ Applied Deep Learning - A4 Exam Cheat Sheet

## ğŸ§  **1. NEURAL NETWORK FUNDAMENTALS**

### **Basic Architecture**

- **Forward Pass**: `y = Ïƒ(Wx + b)`
- **Loss Function**: `L = 1/n Î£(y_true - y_pred)Â²` (MSE) or `CrossEntropy`
- **Backpropagation**: `âˆ‚L/âˆ‚w = x(y_pred - y_true)`, `âˆ‚L/âˆ‚b = (y_pred - y_true)`
- **Parameter Update**: `w = w - Î±âˆ‡w`, `b = b - Î±âˆ‡b`

### **ANN Visual Architecture**

```
Input Layer    Hidden Layer    Output Layer
   xâ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ hâ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ yâ‚
   xâ‚‚ â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€ hâ‚‚ â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€ yâ‚‚
   xâ‚ƒ â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€ hâ‚ƒ â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€ yâ‚ƒ
     (Dense connections)   (Dense connections)
```

### **Activation Functions**

- **ReLU**: `f(x) = max(0,x)` - Most common, solves vanishing gradients
- **Sigmoid**: `f(x) = 1/(1+e^-x)` - Output [0,1], saturates
- **Tanh**: `f(x) = (e^x-e^-x)/(e^x+e^-x)` - Output [-1,1], zero-centered
- **Softmax**: `f(x_i) = e^x_i/Î£e^x_j` - Multi-class classification

### **Batch Normalization**

- **Formula**: `BN(x) = Î³((x-Î¼)/Ïƒ) + Î²`
- **Benefits**: Faster training, higher learning rates, regularization
- **When**: Usually before activation function
- **Why Works**: Smooths optimization landscape, reduces layer interdependence

## ğŸ—ï¸ **2. CNN ARCHITECTURE**

### **CNN Visual Architecture**

```
Input Image â†’ [Conv2d] â†’ [BatchNorm] â†’ [ReLU] â†’ [MaxPool] â†’ [Conv2d] â†’ ... â†’ [Flatten] â†’ [Linear] â†’ Output
   28Ã—28         5Ã—5 filters    BN         ReLU      2Ã—2        3Ã—3                                   10 classes
             â†“                              â†“         â†“
         Feature Maps              Activation    Downsampling
```

### **Convolution Operation Visual**

```
Input:     Filter:    Output:
1 2 3      1 0        (1Ã—1)+(2Ã—0)+(4Ã—1)+(5Ã—0) = 5
4 5 6   *  0 1    =   (2Ã—1)+(3Ã—0)+(5Ã—1)+(6Ã—0) = 7
7 8 9                 (4Ã—1)+(5Ã—0)+(7Ã—1)+(8Ã—0) = 11
```

### **Core Operations**

- **Convolution**: `(f*g)[n] = Î£f[m]g[n-m]` (actually cross-correlation)
- **Padding**: 'SAME' keeps size, 'VALID' reduces size
- **Stride**: Step size for filter movement
- **Pooling**: MaxPool takes maximum, AvgPool takes average

### **Why CNNs Excel at Images**

1. **Weight Sharing**: Same filter applied everywhere â†’ translation invariance
2. **Local Connectivity**: Each neuron sees small spatial region
3. **Hierarchical Features**: Edges â†’ Shapes â†’ Objects
4. **Parameter Efficiency**: ~10x fewer parameters than FC
5. **Spatial Preservation**: Maintains image structure

### **Classic Architectures**

- **LeNet-5** (1998): Convâ†’Poolâ†’Convâ†’Poolâ†’FC (60K params)
- **AlexNet** (2012): ReLU, Dropout, 60M params
- **VGG** (2014): Deep with 3Ã—3 filters only
- **ResNet** (2015): Skip connections solve vanishing gradients

### **Dimensions**

- **Input**: `(B, C, H, W)`
- **Conv2d**: `out = (in + 2p - k)/s + 1`
- **MaxPool**: Reduces spatial dimensions

## ğŸ”„ **3. RNN ARCHITECTURES**

### **Vanilla RNN Visual**

```
Input:  xâ‚ â†’ xâ‚‚ â†’ xâ‚ƒ â†’ xâ‚„
         â†“    â†“    â†“    â†“
State:  hâ‚ â†’ hâ‚‚ â†’ hâ‚ƒ â†’ hâ‚„
         â†“    â†“    â†“    â†“
Output: yâ‚   yâ‚‚   yâ‚ƒ   yâ‚„

h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b)
```

### **LSTM Architecture**

```
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ C_{t-1} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ C_t â”€â”€â”
      â”‚                  â†“                    â†‘     â”‚
   â”Œâ”€â”€â–¼â”€â”€â”         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â–²â”€â”€â”€â”€â” â”‚
h_{t-1}â”‚ f_t â”‚ Ã—   â”‚  tanh   â”‚    Ã—    â”‚   i_t   â”‚ â”‚
   â””â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
      â†‘                  â†‘                    â†‘     â”‚
   â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”  â”‚
   â”‚          x_t (input)                       â”‚  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                           â”‚                       â”‚
                        â”Œâ”€â”€â–¼â”€â”€â”                   â”‚
                        â”‚ o_t â”‚                   â”‚
                        â””â”€â”€â–¼â”€â”€â”˜                   â”‚
                           Ã—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â†’ h_t
                        tanh(C_t)
```

### **LSTM Gates**

### **Vanilla RNN**

- **Hidden State**: `h_t = tanh(W_hhÂ·h_{t-1} + W_ihÂ·x_t + b)`
- **Output**: `y_t = W_hyÂ·h_t + b_y`
- **Problem**: Vanishing gradients for long sequences

### **LSTM (Long Short-Term Memory)**

- **Forget Gate**: `f_t = Ïƒ(W_fÂ·[h_{t-1}, x_t] + b_f)`
- **Input Gate**: `i_t = Ïƒ(W_iÂ·[h_{t-1}, x_t] + b_i)`
- **Candidate**: `CÌƒ_t = tanh(W_CÂ·[h_{t-1}, x_t] + b_C)`
- **Cell State**: `C_t = f_t * C_{t-1} + i_t * CÌƒ_t`
- **Output Gate**: `o_t = Ïƒ(W_oÂ·[h_{t-1}, x_t] + b_o)`
- **Hidden State**: `h_t = o_t * tanh(C_t)`

### **GRU (Gated Recurrent Unit)**

- **Reset Gate**: `r_t = Ïƒ(W_rÂ·[h_{t-1}, x_t])`
- **Update Gate**: `z_t = Ïƒ(W_zÂ·[h_{t-1}, x_t])`
- **New Memory**: `hÌƒ_t = tanh(WÂ·[r_t * h_{t-1}, x_t])`
- **Hidden State**: `h_t = (1-z_t) * h_{t-1} + z_t * hÌƒ_t`

### **When to Use**

- **Vanilla RNN**: Short sequences (<10 steps)
- **LSTM**: Long sequences, need long-term dependencies
- **GRU**: Efficient alternative to LSTM, 25% fewer parameters
- **Bidirectional**: When full context available (not real-time)

## âš–ï¸ **4. BIAS-VARIANCE TRADEOFF**

### **Mathematical Definition**

`Total Error = BiasÂ² + Variance + Irreducible Error`

### **Symptoms & Solutions**

| Problem           | Train Acc | Val Acc | Gap   | Solution           |
| ----------------- | --------- | ------- | ----- | ------------------ |
| **High Bias**     | Low       | Low     | Small | â†‘ Model complexity |
| **High Variance** | High      | Low     | Large | â†‘ Regularization   |
| **Good Fit**      | Medium    | Medium  | Small | âœ“ Keep current     |

### **Bias vs Variance**

- **High Bias**: Underfitting, model too simple
- **High Variance**: Overfitting, model too complex
- **Goal**: Minimize total error, not individual components

## ğŸ”„ **5. TRANSFER LEARNING**

### **Strategies**

1. **Feature Extraction**: Freeze pretrained weights, train classifier only
2. **Fine-tuning**: Unfreeze all/some layers, use lower LR for early layers
3. **Progressive Unfreezing**: Gradually unfreeze layers during training

### **When to Use**

- **Small dataset + similar domain** â†’ Feature extraction
- **Large dataset + different domain** â†’ Fine-tuning
- **Very small dataset** â†’ Feature extraction + data augmentation

### **Learning Rates**

- **Pretrained layers**: 1e-5 to 1e-4
- **New classifier**: 1e-3 to 1e-2
- **Discriminative LR**: Early layers < Later layers

## ğŸ› ï¸ **6. OPTIMIZATION & TRAINING**

### **Optimizers - Complete Equations**

#### **1. Stochastic Gradient Descent (SGD)**

```
Basic SGD:
w_{t+1} = w_t - Î±âˆ‡L(w_t)

SGD with Momentum:
v_t = Î²v_{t-1} + âˆ‡L(w_t)
w_{t+1} = w_t - Î±v_t

where:
- Î± = learning rate
- Î² = momentum coefficient (typically 0.9)
- v_t = velocity (momentum term)
```

#### **2. Adagrad (Adaptive Gradient)**

```
G_t = G_{t-1} + âˆ‡L(w_t)Â²   (element-wise)
w_{t+1} = w_t - (Î±/âˆš(G_t + Îµ)) Â· âˆ‡L(w_t)

where:
- G_t = accumulated squared gradients
- Îµ = small constant (1e-8) for numerical stability
- Problem: G_t grows monotonically â†’ learning rate â†’ 0
```

#### **3. RMSprop (Root Mean Square Propagation)**

```
G_t = Î³G_{t-1} + (1-Î³)âˆ‡L(w_t)Â²   (element-wise)
w_{t+1} = w_t - (Î±/âˆš(G_t + Îµ)) Â· âˆ‡L(w_t)

where:
- Î³ = decay rate (typically 0.9)
- Fixes Adagrad's problem with exponential moving average
```

#### **4. Adam (Adaptive Moment Estimation)**

```
m_t = Î²â‚m_{t-1} + (1-Î²â‚)âˆ‡L(w_t)        [1st moment - mean]
v_t = Î²â‚‚v_{t-1} + (1-Î²â‚‚)âˆ‡L(w_t)Â²       [2nd moment - variance]

Bias correction:
mÌ‚_t = m_t/(1-Î²â‚áµ—)
vÌ‚_t = v_t/(1-Î²â‚‚áµ—)

Update:
w_{t+1} = w_t - Î±(mÌ‚_t/âˆš(vÌ‚_t + Îµ))

Default hyperparameters:
- Î± = 0.001, Î²â‚ = 0.9, Î²â‚‚ = 0.999, Îµ = 1e-8
```

#### **5. AdamW (Adam with Weight Decay)**

```
m_t = Î²â‚m_{t-1} + (1-Î²â‚)âˆ‡L(w_t)
v_t = Î²â‚‚v_{t-1} + (1-Î²â‚‚)âˆ‡L(w_t)Â²

mÌ‚_t = m_t/(1-Î²â‚áµ—)
vÌ‚_t = v_t/(1-Î²â‚‚áµ—)

w_{t+1} = w_t - Î±(mÌ‚_t/âˆš(vÌ‚_t + Îµ)) - Î±Î»w_t

where:
- Î» = weight decay coefficient (decoupled from gradient)
- More effective than L2 regularization in Adam
```

#### **6. Adadelta**

```
E[gÂ²]_t = ÏE[gÂ²]_{t-1} + (1-Ï)âˆ‡L(w_t)Â²
RMS[g]_t = âˆš(E[gÂ²]_t + Îµ)

Î”w_t = -(RMS[Î”w]_{t-1}/RMS[g]_t) Â· âˆ‡L(w_t)
w_{t+1} = w_t + Î”w_t

E[Î”wÂ²]_t = ÏE[Î”wÂ²]_{t-1} + (1-Ï)Î”w_tÂ²
RMS[Î”w]_t = âˆš(E[Î”wÂ²]_t + Îµ)

where:
- Ï = decay constant (typically 0.95)
- No learning rate needed!
```

#### **7. Nadam (Nesterov Adam)**

```
m_t = Î²â‚m_{t-1} + (1-Î²â‚)âˆ‡L(w_t)
v_t = Î²â‚‚v_{t-1} + (1-Î²â‚‚)âˆ‡L(w_t)Â²

mÌ‚_t = m_t/(1-Î²â‚áµ—)
vÌ‚_t = v_t/(1-Î²â‚‚áµ—)

Nesterov momentum:
w_{t+1} = w_t - Î±[Î²â‚mÌ‚_t + ((1-Î²â‚)/(1-Î²â‚áµ—))âˆ‡L(w_t)]/âˆš(vÌ‚_t + Îµ)
```

#### **8. RAdam (Rectified Adam)**

```
Standard Adam updates with rectification term:

Ïâˆ = 2/(1-Î²â‚‚) - 1
Ï_t = Ïâˆ - 2tÎ²â‚‚áµ—/(1-Î²â‚‚áµ—)

if Ï_t > 4:
    r_t = âˆš[(Ï_t-4)(Ï_t-2)Ïâˆ]/[(Ïâˆ-4)(Ïâˆ-2)Ï_t]
    w_{t+1} = w_t - Î±r_t(mÌ‚_t/âˆš(vÌ‚_t + Îµ))
else:
    w_{t+1} = w_t - Î±mÌ‚_t

where r_t is the rectification term that stabilizes early training
```

### **Optimizer Comparison Table**

| Optimizer        | Learning Rate | Momentum      | Adaptive LR    | Memory | Best For                                 |
| ---------------- | ------------- | ------------- | -------------- | ------ | ---------------------------------------- |
| **SGD**          | Manual        | Optional      | âŒ             | Low    | Simple problems, fine-tuning             |
| **SGD+Momentum** | Manual        | âœ…            | âŒ             | Low    | CNNs, when convergence matters           |
| **Adagrad**      | Adaptive      | âŒ            | âœ…             | Medium | Sparse data, early stopping              |
| **RMSprop**      | Adaptive      | âŒ            | âœ…             | Medium | RNNs, non-stationary objectives          |
| **Adam**         | Adaptive      | âœ…            | âœ…             | High   | **Default choice**, most problems        |
| **AdamW**        | Adaptive      | âœ…            | âœ…             | High   | Transformers, when regularization needed |
| **Nadam**        | Adaptive      | âœ… (Nesterov) | âœ…             | High   | When faster convergence needed           |
| **RAdam**        | Adaptive      | âœ…            | âœ… (Rectified) | High   | Unstable early training                  |

### **Quick Optimizer Selection Guide**

```
Problem Type?
â”œâ”€â”€ Small dataset/Simple model â†’ SGD + Momentum
â”œâ”€â”€ Computer Vision (CNN) â†’ SGD + Momentum OR Adam
â”œâ”€â”€ NLP/Transformers â†’ AdamW
â”œâ”€â”€ RNN/LSTM â†’ RMSprop OR Adam
â”œâ”€â”€ Unstable training â†’ RAdam
â”œâ”€â”€ Need fast convergence â†’ Nadam
â””â”€â”€ Default/Unsure â†’ Adam
```

### **Learning Rate Guidelines**

- **SGD**: 0.01 - 0.1
- **Adam/AdamW**: 0.001 - 0.003
- **RMSprop**: 0.001
- **Rule of thumb**: Start with defaults, then tune
- **Scheduling**: Reduce LR when loss plateaus

### **Regularization**

- **L1 (Lasso)**: `Loss + Î»|w|` - Sparsity, feature selection
- **L2 (Ridge)**: `Loss + Î»wÂ²` - Weight decay, smooth
- **Dropout**: Randomly zero neurons during training
- **Batch Norm**: Normalize layer inputs, stabilizes training

### **Training Techniques**

- **Gradient Clipping**: `torch.nn.utils.clip_grad_norm_(params, 1.0)`
- **Early Stopping**: Stop when validation loss stops improving
- **Learning Rate Scheduling**: ReduceLROnPlateau, Cosine annealing

### **Hyperparameters**

- **Batch Size**: 32, 64, 128
- **Hidden Units**: 64, 128, 256
- **Dropout**: 0.2-0.5
- **Weight Decay**: 1e-4 to 1e-5

## ğŸ“Š **7. PRACTICAL TIPS**

### **Model Selection Guide**

```
Data Type?
â”œâ”€â”€ Tabular â†’ ANN
â”œâ”€â”€ Images â†’ CNN
â””â”€â”€ Sequential
    â”œâ”€â”€ Short â†’ RNN
    â”œâ”€â”€ Long + Accuracy â†’ LSTM
    â””â”€â”€ Long + Speed â†’ GRU
```

### **PyTorch Training Loop**

```python
for epoch in range(epochs):
    for batch_x, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
```

### **Common Issues & Fixes**

- **Exploding Gradients** â†’ Gradient clipping
- **Vanishing Gradients** â†’ LSTM/GRU, skip connections
- **Overfitting** â†’ Dropout, regularization, more data
- **Underfitting** â†’ Larger model, train longer
- **Slow Convergence** â†’ Higher LR, better optimizer

### **Layer Counting Rule**

Count only layers with **learnable parameters**:

- âœ… nn.Linear, nn.Conv2d, nn.LSTM
- âŒ ReLU, Dropout, MaxPool (no parameters)

## ğŸ¤– **8. MODERN ARCHITECTURES**

### **Autoencoders**

```
Encoder:    Input â†’ Hidden (Bottleneck) â†’ Latent Space
Decoder:    Latent Space â†’ Hidden â†’ Reconstructed Output
Goal: Minimize reconstruction loss: L = ||x - xÌ‚||Â²
```

- **Use Cases**: Dimensionality reduction, denoising, anomaly detection
- **Variational AE (VAE)**: Adds probabilistic latent space

### **Attention Mechanisms**

- **Formula**: `Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V`
- **Self-Attention**: Q, K, V all from same input
- **Purpose**: Focus on relevant parts of input sequence

### **Transformer Architecture**

```
Input â†’ Positional Encoding â†’ Multi-Head Attention â†’ LayerNorm â†’ FFN â†’ Output
                                    â†‘                               â†‘
                              (Residual Connection)        (Residual Connection)
```

- **Key Innovation**: Attention is all you need (no RNNs!)
- **Benefits**: Parallelizable, handles long sequences better

### **Generative Models**

- **GAN**: Generator vs Discriminator (adversarial training)
- **VAE**: Variational Autoencoders (probabilistic approach)
- **Diffusion**: Gradual denoising process

## ğŸ§ª **9. MODERN TECHNIQUES**

### **Layer Normalization**

- **Formula**: `LN(x) = Î³((x-Î¼)/Ïƒ) + Î²` (normalize across features)
- **vs BatchNorm**: Works per sample, better for RNNs/Transformers

### **Residual Connections**

- **Formula**: `y = F(x) + x` (skip connection)
- **Purpose**: Solves vanishing gradients in very deep networks

### **Advanced Regularization**

- **Label Smoothing**: Soft targets instead of hard 0/1
- **Mixup**: Train on weighted combinations of samples
- **Cutout**: Random masking of image patches

### **Exam Memory Aids**

- **BIAS = BAD + SIMPLE** vs **VARIANCE = VOLATILE + COMPLEX**
- **CNN**: Spatial patterns, weight sharing, hierarchical
- **RNN**: Sequential data, hidden state memory
- **LSTM**: Gates control information flow
- **Transfer Learning**: Start with pretrained, adapt to new task
- **Attention**: Focus mechanism, "what to pay attention to"
- **Transformers**: All attention, highly parallelizable

## ğŸ® **10. REINFORCEMENT LEARNING**

### **RL Agent Types**

```
1. Model-Free Agents:    Learn directly from experience
   â€¢ Value-based:        Q-Learning, DQN
   â€¢ Policy-based:       REINFORCE, Actor-Critic
   â€¢ Actor-Critic:       Combines value + policy

2. Model-Based Agents:   Learn environment model first
   â€¢ Planning-based:     Use model to plan actions
   â€¢ Dyna-Q:            Mix model learning + direct experience
```

### **Basic RL Mechanism**

```
Environment â†â†’ Agent
     â†“           â†‘
   State(s)   Action(a)
     â†“           â†‘
  Reward(r) â†’ Learning

Goal: Maximize cumulative reward Î£Î³áµ—râ‚œ
```

- **MDP Components**: States (S), Actions (A), Rewards (R), Transitions (P), Discount (Î³)
- **Policy Ï€(a|s)**: Probability of action a in state s
- **Value Function V(s)**: Expected return from state s
- **Q-Function Q(s,a)**: Expected return from state s, action a

### **Q-Learning Algorithm**

```
Q-Table Update:
Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]
                      â†‘              â†‘        â†‘
                   Learning    Best future   Current
                     Rate       Q-value      Q-value
```

**Steps:**

1. Observe state s
2. Choose action a (Îµ-greedy: explore vs exploit)
3. Execute action, get reward r and next state s'
4. Update Q(s,a) using Bellman equation
5. Repeat until convergence

### **Deep Q-Learning (DQN)**

```
Neural Network Architecture:
State â†’ [Conv/FC Layers] â†’ Q-values for all actions
  s                          Q(s,aâ‚), Q(s,aâ‚‚), ..., Q(s,aâ‚™)
```

**Key Innovations:**

- **Experience Replay**: Store (s,a,r,s') in buffer, sample random batches
- **Target Network**: Separate network for stable targets (update periodically)
- **Loss Function**: `L = (r + Î³ max Q_target(s',a') - Q(s,a))Â²`

### **DQN Algorithm**

```python
# Pseudocode
for episode in episodes:
    state = env.reset()
    for step in steps:
        # Îµ-greedy action selection
        if random() < Îµ:
            action = random_action()
        else:
            action = argmax(Q(state))

        next_state, reward, done = env.step(action)
        buffer.store(state, action, reward, next_state, done)

        # Train network
        if len(buffer) > batch_size:
            batch = buffer.sample(batch_size)
            target = r + Î³ * max(Q_target(s'))
            loss = (target - Q(s,a))Â²
            optimize(loss)

        # Update target network periodically
        if step % C == 0:
            Q_target = Q.copy()
```

### **Deep RL Techniques**

**1. Policy Gradient Methods**

- **REINFORCE**: `âˆ‡J(Î¸) = Î£ âˆ‡log Ï€(a|s) Ã— R`
- **Actor-Critic**: Actor (policy) + Critic (value function)
- **A3C**: Asynchronous Actor-Critic with multiple workers

**2. Advanced Value Methods**

- **Double DQN**: Separate action selection and evaluation
- **Dueling DQN**: Split Q(s,a) = V(s) + A(s,a)
- **Prioritized Experience Replay**: Sample important transitions more

**3. Policy Optimization**

- **PPO**: Proximal Policy Optimization (stable policy updates)
- **TRPO**: Trust Region Policy Optimization
- **SAC**: Soft Actor-Critic (maximum entropy RL)

### **RL Problem Types**

- **Discrete Actions**: DQN, Q-Learning
- **Continuous Actions**: Policy gradients, Actor-Critic
- **Multi-Agent**: Independent learners, centralized training
- **Partial Observability**: POMDP, recurrent policies

### **Exploration Strategies**

- **Îµ-greedy**: Random action with probability Îµ
- **UCB**: Upper Confidence Bound
- **Thompson Sampling**: Bayesian exploration
- **Curiosity-driven**: Intrinsic motivation

### **RL Visual Architecture**

```
DQN Architecture:
Input State â†’ CNN/FC â†’ Hidden Layers â†’ Q-values Output
   84Ã—84Ã—4      Conv      Dense         |A| neurons
    â†“            â†“         â†“              â†“
  Atari        Feature   Value       Action Selection
  Frames      Extraction Function    (argmax or Îµ-greedy)

Actor-Critic:
State â†’ Shared Network â†’ Actor (Policy Ï€)
                      â†’ Critic (Value V)
```

---

**ğŸ“ Remember**: Understand the WHY behind each technique, not just the equations!
