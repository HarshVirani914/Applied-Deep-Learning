{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e04dba8",
   "metadata": {},
   "source": [
    "# Simple PyTorch Neural Network Implementations\n",
    "\n",
    "## üéØ **Exam Preparation - Core Neural Network Concepts**\n",
    "\n",
    "This notebook contains **simple, clean implementations** of essential neural network architectures using PyTorch. Perfect for exam revision and understanding core concepts.\n",
    "\n",
    "### **üìö What's Covered:**\n",
    "1. **üß† Artificial Neural Network (ANN)** - Basic feedforward networks\n",
    "2. **üñºÔ∏è Convolutional Neural Network (CNN)** - Image processing networks  \n",
    "3. **üîÑ Recurrent Neural Network (RNN)** - Sequential data processing\n",
    "4. **üßÆ Long Short-Term Memory (LSTM)** - Advanced sequence modeling\n",
    "5. **‚ö° Gated Recurrent Unit (GRU)** - Efficient sequence processing\n",
    "6. **üèãÔ∏è Training & Evaluation** - Complete training pipelines\n",
    "\n",
    "### **üéì Learning Objectives:**\n",
    "- Understand **architecture fundamentals** of each network type\n",
    "- Learn **PyTorch implementation patterns**\n",
    "- Master **forward pass mechanics**\n",
    "- Practice **loss calculation and backpropagation**\n",
    "- Compare **performance characteristics**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e7db3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Data handling and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# Utilities\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üì¶ PyTorch version: {torch.__version__}\")\n",
    "print(f\"üéØ Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be0d38b",
   "metadata": {},
   "source": [
    "## üìä Sample Datasets\n",
    "\n",
    "We'll create simple synthetic datasets to test our neural network implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5788c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Datasets for Testing Neural Networks\n",
    "\n",
    "def create_classification_data(n_samples=1000, n_features=10, n_classes=3):\n",
    "    \"\"\"Create synthetic classification dataset for ANN.\"\"\"\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_classes=n_classes,\n",
    "        n_informative=n_features//2,\n",
    "        n_redundant=0,\n",
    "        n_clusters_per_class=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    return torch.FloatTensor(X), torch.LongTensor(y)\n",
    "\n",
    "def create_regression_data(n_samples=1000, n_features=5):\n",
    "    \"\"\"Create synthetic regression dataset.\"\"\"\n",
    "    X, y = make_regression(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        noise=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Normalize features and targets\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    X = scaler_X.fit_transform(X)\n",
    "    y = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return torch.FloatTensor(X), torch.FloatTensor(y)\n",
    "\n",
    "def create_image_data(n_samples=1000, img_size=28):\n",
    "    \"\"\"Create synthetic image data for CNN (simplified MNIST-like).\"\"\"\n",
    "    # Create simple geometric patterns\n",
    "    X = torch.randn(n_samples, 1, img_size, img_size)\n",
    "    y = torch.randint(0, 3, (n_samples,))\n",
    "    \n",
    "    # Add some pattern structure\n",
    "    for i in range(n_samples):\n",
    "        if y[i] == 0:  # Circles\n",
    "            center = img_size // 2\n",
    "            radius = img_size // 4\n",
    "            for r in range(img_size):\n",
    "                for c in range(img_size):\n",
    "                    if (r - center)**2 + (c - center)**2 <= radius**2:\n",
    "                        X[i, 0, r, c] = 1.0\n",
    "        elif y[i] == 1:  # Horizontal lines\n",
    "            start_row = img_size // 3\n",
    "            end_row = 2 * img_size // 3\n",
    "            X[i, 0, start_row:end_row, :] = 1.0\n",
    "        else:  # Diagonal lines\n",
    "            for k in range(img_size):\n",
    "                if k < img_size:\n",
    "                    X[i, 0, k, k] = 1.0\n",
    "                if img_size - 1 - k >= 0:\n",
    "                    X[i, 0, k, img_size - 1 - k] = 1.0\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def create_sequence_data(n_samples=1000, seq_len=20, n_features=5):\n",
    "    \"\"\"Create synthetic sequence data for RNN/LSTM/GRU.\"\"\"\n",
    "    X = torch.randn(n_samples, seq_len, n_features)\n",
    "    \n",
    "    # Create targets based on sequence patterns\n",
    "    # Class 0: Increasing trend, Class 1: Decreasing trend, Class 2: Oscillating\n",
    "    y = torch.zeros(n_samples, dtype=torch.long)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Add trend to first feature\n",
    "        if i % 3 == 0:  # Increasing\n",
    "            trend = torch.linspace(0, 2, seq_len)\n",
    "            X[i, :, 0] += trend\n",
    "            y[i] = 0\n",
    "        elif i % 3 == 1:  # Decreasing  \n",
    "            trend = torch.linspace(2, 0, seq_len)\n",
    "            X[i, :, 0] += trend\n",
    "            y[i] = 1\n",
    "        else:  # Oscillating\n",
    "            trend = torch.sin(torch.linspace(0, 4*np.pi, seq_len))\n",
    "            X[i, :, 0] += trend\n",
    "            y[i] = 2\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Create all datasets\n",
    "print(\"üèóÔ∏è Creating datasets...\")\n",
    "\n",
    "# 1. Classification data for ANN\n",
    "X_clf, y_clf = create_classification_data(n_samples=1000, n_features=10, n_classes=3)\n",
    "print(f\"üìà Classification dataset: {X_clf.shape}, {y_clf.shape}\")\n",
    "\n",
    "# 2. Regression data\n",
    "X_reg, y_reg = create_regression_data(n_samples=1000, n_features=5)\n",
    "print(f\"üìä Regression dataset: {X_reg.shape}, {y_reg.shape}\")\n",
    "\n",
    "# 3. Image data for CNN\n",
    "X_img, y_img = create_image_data(n_samples=800, img_size=28)\n",
    "print(f\"üñºÔ∏è Image dataset: {X_img.shape}, {y_img.shape}\")\n",
    "\n",
    "# 4. Sequence data for RNN/LSTM/GRU\n",
    "X_seq, y_seq = create_sequence_data(n_samples=800, seq_len=20, n_features=5)\n",
    "print(f\"üîÑ Sequence dataset: {X_seq.shape}, {y_seq.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖ All datasets created successfully!\")\n",
    "\n",
    "# Visualize sample data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Classification data\n",
    "axes[0,0].scatter(X_clf[:, 0], X_clf[:, 1], c=y_clf, alpha=0.7)\n",
    "axes[0,0].set_title('Classification Data (First 2 Features)')\n",
    "axes[0,0].set_xlabel('Feature 1')\n",
    "axes[0,0].set_ylabel('Feature 2')\n",
    "\n",
    "# Regression data\n",
    "axes[0,1].scatter(X_reg[:, 0], y_reg, alpha=0.7)\n",
    "axes[0,1].set_title('Regression Data')\n",
    "axes[0,1].set_xlabel('Feature 1')\n",
    "axes[0,1].set_ylabel('Target')\n",
    "\n",
    "# Sample image\n",
    "sample_img = X_img[0, 0].numpy()\n",
    "axes[1,0].imshow(sample_img, cmap='gray')\n",
    "axes[1,0].set_title(f'Sample Image (Class {y_img[0]})')\n",
    "axes[1,0].axis('off')\n",
    "\n",
    "# Sample sequence\n",
    "sample_seq = X_seq[0, :, 0].numpy()\n",
    "axes[1,1].plot(sample_seq)\n",
    "axes[1,1].set_title(f'Sample Sequence (Class {y_seq[0]})')\n",
    "axes[1,1].set_xlabel('Time Steps')\n",
    "axes[1,1].set_ylabel('Feature Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df93274",
   "metadata": {},
   "source": [
    "## üß† 1. Simple Artificial Neural Network (ANN)\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Feedforward architecture** with fully connected layers\n",
    "- **Activation functions** (ReLU, Sigmoid, Tanh)\n",
    "- **Backpropagation** for weight updates\n",
    "- **Gradient descent optimization**\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input Layer ‚Üí Hidden Layer(s) ‚Üí Output Layer\n",
    "     ‚Üì              ‚Üì              ‚Üì\n",
    "  Linear         Linear        Linear\n",
    "Transform    + Activation    + Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c6072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Artificial Neural Network Implementation\n",
    "\n",
    "class SimpleANN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Artificial Neural Network for classification.\n",
    "    \n",
    "    Architecture: Input ‚Üí Hidden ‚Üí Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, activation='relu'):\n",
    "        super(SimpleANN, self).__init__()\n",
    "        \n",
    "        # Store architecture info\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)   # Input ‚Üí Hidden\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  # Hidden ‚Üí Output\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()  # Default\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # Input ‚Üí Hidden (with activation)\n",
    "        hidden = self.fc1(x)          # Linear transformation\n",
    "        hidden = self.activation(hidden)  # Non-linear activation\n",
    "        \n",
    "        # Hidden ‚Üí Output (no activation for classification)\n",
    "        output = self.fc2(hidden)     # Final linear transformation\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_info(self):\n",
    "        \"\"\"Return model information.\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        return {\n",
    "            'Architecture': 'Feedforward Neural Network',\n",
    "            'Input Size': self.input_size,\n",
    "            'Hidden Size': self.hidden_size,\n",
    "            'Output Size': self.output_size,\n",
    "            'Total Parameters': total_params,\n",
    "            'Activation': str(self.activation)\n",
    "        }\n",
    "\n",
    "# Create and test ANN\n",
    "print(\"üß† Simple ANN Implementation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Model parameters\n",
    "input_size = X_clf.shape[1]  # Number of features\n",
    "hidden_size = 64\n",
    "output_size = len(torch.unique(y_clf))  # Number of classes\n",
    "\n",
    "# Create model\n",
    "ann_model = SimpleANN(input_size, hidden_size, output_size, activation='relu')\n",
    "\n",
    "# Display model info\n",
    "model_info = ann_model.get_info()\n",
    "for key, value in model_info.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä Model Architecture:\")\n",
    "print(ann_model)\n",
    "\n",
    "# Test forward pass\n",
    "sample_input = X_clf[:5]  # First 5 samples\n",
    "with torch.no_grad():\n",
    "    sample_output = ann_model(sample_input)\n",
    "    \n",
    "print(f\"\\nüîç Forward Pass Test:\")\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {sample_output.shape}\")\n",
    "print(f\"Sample outputs (raw logits):\")\n",
    "for i, output in enumerate(sample_output):\n",
    "    predicted_class = torch.argmax(output).item()\n",
    "    actual_class = y_clf[i].item()\n",
    "    print(f\"  Sample {i+1}: Predicted={predicted_class}, Actual={actual_class}\")\n",
    "\n",
    "print(\"\\n‚úÖ ANN implementation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3898de31",
   "metadata": {},
   "source": [
    "## üñºÔ∏è 2. Simple Convolutional Neural Network (CNN)\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Convolutional layers** for feature extraction\n",
    "- **Pooling layers** for dimensionality reduction\n",
    "- **Feature maps** and **spatial hierarchies**\n",
    "- **Parameter sharing** across spatial locations\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input Image ‚Üí Conv ‚Üí ReLU ‚Üí Pool ‚Üí Conv ‚Üí ReLU ‚Üí Pool ‚Üí Flatten ‚Üí FC ‚Üí Output\n",
    "    ‚Üì          ‚Üì      ‚Üì      ‚Üì      ‚Üì      ‚Üì      ‚Üì       ‚Üì       ‚Üì      ‚Üì\n",
    "  (1,28,28) (32,26,26) ‚Üí (32,13,13) (64,11,11) ‚Üí (64,5,5) ‚Üí (1600) ‚Üí (128) ‚Üí (3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dbb89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Convolutional Neural Network Implementation\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Convolutional Neural Network for image classification.\n",
    "    \n",
    "    Architecture: Conv ‚Üí ReLU ‚Üí Pool ‚Üí Conv ‚Üí ReLU ‚Üí Pool ‚Üí Flatten ‚Üí FC ‚Üí Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels=1, num_classes=3):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Store architecture info\n",
    "        self.input_channels = input_channels\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)  # (1,28,28) ‚Üí (32,28,28)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)                    # (32,28,28) ‚Üí (32,14,14)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)              # (32,14,14) ‚Üí (64,14,14)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)                    # (64,14,14) ‚Üí (64,7,7)\n",
    "        \n",
    "        # Calculate flattened size (depends on input image size)\n",
    "        # For 28x28 input: 64 * 7 * 7 = 3136\n",
    "        self.flattened_size = 64 * 7 * 7\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        # Activation and dropout\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through CNN.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch_size, channels, height, width)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # First convolutional block\n",
    "        x = self.conv1(x)      # Apply convolution\n",
    "        x = self.relu(x)       # Apply activation\n",
    "        x = self.pool1(x)      # Apply pooling\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = self.conv2(x)      # Apply convolution\n",
    "        x = self.relu(x)       # Apply activation\n",
    "        x = self.pool2(x)      # Apply pooling\n",
    "        \n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten: (batch_size, channels*height*width)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)        # First FC layer\n",
    "        x = self.relu(x)       # Activation\n",
    "        x = self.dropout(x)    # Dropout for regularization\n",
    "        x = self.fc2(x)        # Output layer\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_info(self):\n",
    "        \"\"\"Return model information.\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        return {\n",
    "            'Architecture': 'Convolutional Neural Network',\n",
    "            'Input Channels': self.input_channels,\n",
    "            'Output Classes': self.num_classes,\n",
    "            'Total Parameters': total_params,\n",
    "            'Flattened Size': self.flattened_size\n",
    "        }\n",
    "\n",
    "# Create and test CNN\n",
    "print(\"üñºÔ∏è Simple CNN Implementation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Model parameters\n",
    "input_channels = X_img.shape[1]  # Number of channels\n",
    "num_classes = len(torch.unique(y_img))  # Number of classes\n",
    "\n",
    "# Create model\n",
    "cnn_model = SimpleCNN(input_channels, num_classes)\n",
    "\n",
    "# Display model info\n",
    "model_info = cnn_model.get_info()\n",
    "for key, value in model_info.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä Model Architecture:\")\n",
    "print(cnn_model)\n",
    "\n",
    "# Test forward pass\n",
    "sample_input = X_img[:5]  # First 5 images\n",
    "print(f\"\\nüîç Forward Pass Test:\")\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_output = cnn_model(sample_input)\n",
    "    \n",
    "print(f\"Output shape: {sample_output.shape}\")\n",
    "print(f\"Sample outputs (raw logits):\")\n",
    "for i, output in enumerate(sample_output):\n",
    "    predicted_class = torch.argmax(output).item()\n",
    "    actual_class = y_img[i].item()\n",
    "    confidence = torch.softmax(output, dim=0)[predicted_class].item()\n",
    "    print(f\"  Sample {i+1}: Predicted={predicted_class} (conf: {confidence:.3f}), Actual={actual_class}\")\n",
    "\n",
    "# Visualize feature maps\n",
    "def visualize_feature_maps(model, input_image, layer_name='conv1'):\n",
    "    \"\"\"Visualize feature maps from convolutional layers.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get activations from first conv layer\n",
    "    activation = {}\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    # Register hook\n",
    "    if layer_name == 'conv1':\n",
    "        model.conv1.register_forward_hook(get_activation('conv1'))\n",
    "    else:\n",
    "        model.conv2.register_forward_hook(get_activation('conv2'))\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        _ = model(input_image.unsqueeze(0))\n",
    "    \n",
    "    # Get feature maps\n",
    "    feature_maps = activation[layer_name][0]  # Remove batch dimension\n",
    "    \n",
    "    # Plot first 8 feature maps\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(min(8, feature_maps.shape[0])):\n",
    "        axes[i].imshow(feature_maps[i].numpy(), cmap='viridis')\n",
    "        axes[i].set_title(f'Feature Map {i+1}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'{layer_name.upper()} Feature Maps')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize feature maps for first sample\n",
    "print(f\"\\nüé® Visualizing Feature Maps:\")\n",
    "visualize_feature_maps(cnn_model, X_img[0], 'conv1')\n",
    "\n",
    "print(\"\\n‚úÖ CNN implementation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a396e468",
   "metadata": {},
   "source": [
    "## üîÑ 3. Simple Recurrent Neural Network (RNN)\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Sequential processing** with hidden state memory\n",
    "- **Temporal dependencies** across time steps\n",
    "- **Backpropagation Through Time (BPTT)**\n",
    "- **Vanishing gradient problem** for long sequences\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "h‚ÇÄ ‚Üí [RNN Cell] ‚Üí h‚ÇÅ ‚Üí [RNN Cell] ‚Üí h‚ÇÇ ‚Üí ... ‚Üí h‚Çú ‚Üí [Linear] ‚Üí Output\n",
    "     ‚Üë    ‚Üì              ‚Üë    ‚Üì              ‚Üë    ‚Üì\n",
    "    x‚ÇÅ   o‚ÇÅ             x‚ÇÇ   o‚ÇÇ             x‚Çú   o‚Çú\n",
    "\n",
    "Where: h‚Çú = tanh(W·µ¢‚Çïx‚Çú + W‚Çï‚Çïh‚Çú‚Çã‚ÇÅ + b‚Çï)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50283402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Recurrent Neural Network Implementation\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Recurrent Neural Network for sequence classification.\n",
    "    \n",
    "    Architecture: RNN ‚Üí Linear ‚Üí Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        \n",
    "        # Store architecture info\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,  # Input shape: (batch, seq, feature)\n",
    "            nonlinearity='tanh'  # Default activation\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass through RNN.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, input_size)\n",
    "            hidden: Initial hidden state (optional)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor (batch_size, output_size)\n",
    "            Final hidden state\n",
    "        \"\"\"\n",
    "        # RNN forward pass\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        # Hidden: (num_layers, batch_size, hidden_size)\n",
    "        rnn_out, hidden_final = self.rnn(x, hidden)\n",
    "        \n",
    "        # Use the last time step output for classification\n",
    "        # Take output from last time step: (batch_size, hidden_size)\n",
    "        last_output = rnn_out[:, -1, :]\n",
    "        \n",
    "        # Apply linear layer to get class predictions\n",
    "        output = self.fc(last_output)\n",
    "        \n",
    "        return output, hidden_final\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize hidden state with zeros.\"\"\"\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "    \n",
    "    def get_info(self):\n",
    "        \"\"\"Return model information.\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        return {\n",
    "            'Architecture': 'Recurrent Neural Network',\n",
    "            'Input Size': self.input_size,\n",
    "            'Hidden Size': self.hidden_size,\n",
    "            'Output Size': self.output_size,\n",
    "            'Num Layers': self.num_layers,\n",
    "            'Total Parameters': total_params\n",
    "        }\n",
    "\n",
    "# Create and test RNN\n",
    "print(\"üîÑ Simple RNN Implementation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Model parameters\n",
    "input_size = X_seq.shape[2]      # Number of features per time step\n",
    "hidden_size = 32\n",
    "output_size = len(torch.unique(y_seq))  # Number of classes\n",
    "seq_len = X_seq.shape[1]         # Sequence length\n",
    "\n",
    "# Create model\n",
    "rnn_model = SimpleRNN(input_size, hidden_size, output_size, num_layers=1)\n",
    "\n",
    "# Display model info\n",
    "model_info = rnn_model.get_info()\n",
    "for key, value in model_info.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä Model Architecture:\")\n",
    "print(rnn_model)\n",
    "\n",
    "# Test forward pass\n",
    "sample_input = X_seq[:5]  # First 5 sequences\n",
    "batch_size = sample_input.shape[0]\n",
    "\n",
    "print(f\"\\nüîç Forward Pass Test:\")\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "\n",
    "# Initialize hidden state\n",
    "hidden_init = rnn_model.init_hidden(batch_size)\n",
    "print(f\"Initial hidden shape: {hidden_init.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_output, final_hidden = rnn_model(sample_input, hidden_init)\n",
    "    \n",
    "print(f\"Output shape: {sample_output.shape}\")\n",
    "print(f\"Final hidden shape: {final_hidden.shape}\")\n",
    "\n",
    "print(f\"Sample outputs (raw logits):\")\n",
    "for i, output in enumerate(sample_output):\n",
    "    predicted_class = torch.argmax(output).item()\n",
    "    actual_class = y_seq[i].item()\n",
    "    confidence = torch.softmax(output, dim=0)[predicted_class].item()\n",
    "    print(f\"  Sample {i+1}: Predicted={predicted_class} (conf: {confidence:.3f}), Actual={actual_class}\")\n",
    "\n",
    "# Demonstrate hidden state evolution\n",
    "def visualize_hidden_states(model, sequence):\n",
    "    \"\"\"Visualize how hidden states evolve over time.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get hidden states at each time step\n",
    "    hidden_states = []\n",
    "    hidden = model.init_hidden(1)  # Batch size 1\n",
    "    \n",
    "    sequence = sequence.unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for t in range(sequence.shape[1]):  # For each time step\n",
    "            # Forward pass for single time step\n",
    "            input_t = sequence[:, t:t+1, :]  # (1, 1, input_size)\n",
    "            _, hidden = model.rnn(input_t, hidden)\n",
    "            hidden_states.append(hidden[0, 0, :].numpy())  # Remove batch and layer dims\n",
    "    \n",
    "    hidden_states = np.array(hidden_states)  # (seq_len, hidden_size)\n",
    "    \n",
    "    # Plot hidden state evolution\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot all hidden dimensions over time\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.imshow(hidden_states.T, aspect='auto', cmap='viridis')\n",
    "    plt.colorbar(label='Activation Value')\n",
    "    plt.title('Hidden State Evolution Over Time')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Hidden Dimensions')\n",
    "    \n",
    "    # Plot mean hidden state magnitude over time\n",
    "    plt.subplot(2, 1, 2)\n",
    "    hidden_magnitudes = np.linalg.norm(hidden_states, axis=1)\n",
    "    plt.plot(hidden_magnitudes, 'b-', linewidth=2)\n",
    "    plt.title('Hidden State Magnitude Over Time')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('||h(t)||')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize hidden states for first sequence\n",
    "print(f\"\\nüé® Visualizing Hidden State Evolution:\")\n",
    "visualize_hidden_states(rnn_model, X_seq[0])\n",
    "\n",
    "print(\"\\n‚úÖ RNN implementation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa557adb",
   "metadata": {},
   "source": [
    "## üßÆ 4. Simple Long Short-Term Memory (LSTM)\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Memory cells** with controlled information flow\n",
    "- **Gating mechanisms** (forget, input, output gates)\n",
    "- **Long-term dependency** handling\n",
    "- **Gradient flow** through additive cell state\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "    Input Gate     Forget Gate    Output Gate\n",
    "        ‚Üì              ‚Üì              ‚Üì\n",
    "    œÉ(W·µ¢[h,x]+b·µ¢)  œÉ(Wf[h,x]+bf)  œÉ(Wo[h,x]+bo)\n",
    "        ‚Üì              ‚Üì              ‚Üì\n",
    "       i_t  ‚Üê‚Üí  CÃÉ_t  ‚Üê‚Üí  f_t         o_t\n",
    "                 ‚Üì                   ‚Üì\n",
    "             C_t = f_t*C_{t-1} + i_t*CÃÉ_t\n",
    "                               ‚Üì\n",
    "                         h_t = o_t * tanh(C_t)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74501a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Long Short-Term Memory Implementation\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple LSTM Network for sequence classification.\n",
    "    \n",
    "    Architecture: LSTM ‚Üí Linear ‚Üí Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        \n",
    "        # Store architecture info\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,  # Input shape: (batch, seq, feature)\n",
    "            dropout=0.2 if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass through LSTM.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, input_size)\n",
    "            hidden: Tuple of (h_0, c_0) initial states (optional)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor (batch_size, output_size)\n",
    "            Final hidden and cell states\n",
    "        \"\"\"\n",
    "        # LSTM forward pass\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        # Hidden: (h_n, c_n) where each is (num_layers, batch_size, hidden_size)\n",
    "        lstm_out, hidden_final = self.lstm(x, hidden)\n",
    "        \n",
    "        # Use the last time step output for classification\n",
    "        # Take output from last time step: (batch_size, hidden_size)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Apply linear layer to get class predictions\n",
    "        output = self.fc(last_output)\n",
    "        \n",
    "        return output, hidden_final\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize hidden and cell states with zeros.\"\"\"\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return (h_0, c_0)\n",
    "    \n",
    "    def get_info(self):\n",
    "        \"\"\"Return model information.\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        return {\n",
    "            'Architecture': 'Long Short-Term Memory',\n",
    "            'Input Size': self.input_size,\n",
    "            'Hidden Size': self.hidden_size,\n",
    "            'Output Size': self.output_size,\n",
    "            'Num Layers': self.num_layers,\n",
    "            'Total Parameters': total_params\n",
    "        }\n",
    "\n",
    "# Create and test LSTM\n",
    "print(\"üßÆ Simple LSTM Implementation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Model parameters (same as RNN for comparison)\n",
    "input_size = X_seq.shape[2]\n",
    "hidden_size = 32\n",
    "output_size = len(torch.unique(y_seq))\n",
    "\n",
    "# Create model\n",
    "lstm_model = SimpleLSTM(input_size, hidden_size, output_size, num_layers=1)\n",
    "\n",
    "# Display model info\n",
    "model_info = lstm_model.get_info()\n",
    "for key, value in model_info.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä Model Architecture:\")\n",
    "print(lstm_model)\n",
    "\n",
    "# Test forward pass\n",
    "sample_input = X_seq[:5]\n",
    "batch_size = sample_input.shape[0]\n",
    "\n",
    "print(f\"\\nüîç Forward Pass Test:\")\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "\n",
    "# Initialize hidden and cell states\n",
    "hidden_init = lstm_model.init_hidden(batch_size)\n",
    "print(f\"Initial hidden shape: {hidden_init[0].shape}\")\n",
    "print(f\"Initial cell shape: {hidden_init[1].shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_output, final_states = lstm_model(sample_input, hidden_init)\n",
    "    \n",
    "print(f\"Output shape: {sample_output.shape}\")\n",
    "print(f\"Final hidden shape: {final_states[0].shape}\")\n",
    "print(f\"Final cell shape: {final_states[1].shape}\")\n",
    "\n",
    "print(f\"Sample outputs (raw logits):\")\n",
    "for i, output in enumerate(sample_output):\n",
    "    predicted_class = torch.argmax(output).item()\n",
    "    actual_class = y_seq[i].item()\n",
    "    confidence = torch.softmax(output, dim=0)[predicted_class].item()\n",
    "    print(f\"  Sample {i+1}: Predicted={predicted_class} (conf: {confidence:.3f}), Actual={actual_class}\")\n",
    "\n",
    "# Demonstrate LSTM states evolution\n",
    "def visualize_lstm_states(model, sequence):\n",
    "    \"\"\"Visualize LSTM hidden and cell states evolution.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    hidden_states = []\n",
    "    cell_states = []\n",
    "    \n",
    "    # Initialize states\n",
    "    h_t, c_t = model.init_hidden(1)\n",
    "    sequence = sequence.unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for t in range(sequence.shape[1]):\n",
    "            # Forward pass for single time step\n",
    "            input_t = sequence[:, t:t+1, :]\n",
    "            _, (h_t, c_t) = model.lstm(input_t, (h_t, c_t))\n",
    "            \n",
    "            hidden_states.append(h_t[0, 0, :].numpy())\n",
    "            cell_states.append(c_t[0, 0, :].numpy())\n",
    "    \n",
    "    hidden_states = np.array(hidden_states)\n",
    "    cell_states = np.array(cell_states)\n",
    "    \n",
    "    # Plot states evolution\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Hidden states heatmap\n",
    "    im1 = axes[0,0].imshow(hidden_states.T, aspect='auto', cmap='viridis')\n",
    "    axes[0,0].set_title('Hidden States Evolution')\n",
    "    axes[0,0].set_xlabel('Time Steps')\n",
    "    axes[0,0].set_ylabel('Hidden Dimensions')\n",
    "    plt.colorbar(im1, ax=axes[0,0])\n",
    "    \n",
    "    # Cell states heatmap\n",
    "    im2 = axes[0,1].imshow(cell_states.T, aspect='auto', cmap='plasma')\n",
    "    axes[0,1].set_title('Cell States Evolution')\n",
    "    axes[0,1].set_xlabel('Time Steps')\n",
    "    axes[0,1].set_ylabel('Cell Dimensions')\n",
    "    plt.colorbar(im2, ax=axes[0,1])\n",
    "    \n",
    "    # States magnitude over time\n",
    "    hidden_magnitudes = np.linalg.norm(hidden_states, axis=1)\n",
    "    cell_magnitudes = np.linalg.norm(cell_states, axis=1)\n",
    "    \n",
    "    axes[1,0].plot(hidden_magnitudes, 'b-', linewidth=2, label='Hidden State')\n",
    "    axes[1,0].plot(cell_magnitudes, 'r-', linewidth=2, label='Cell State')\n",
    "    axes[1,0].set_title('State Magnitudes Over Time')\n",
    "    axes[1,0].set_xlabel('Time Steps')\n",
    "    axes[1,0].set_ylabel('||State||')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # States comparison\n",
    "    axes[1,1].plot(hidden_states[:, 0], 'b-', alpha=0.7, label='Hidden[0]')\n",
    "    axes[1,1].plot(cell_states[:, 0], 'r-', alpha=0.7, label='Cell[0]')\n",
    "    axes[1,1].plot(hidden_states[:, 1], 'b--', alpha=0.7, label='Hidden[1]')\n",
    "    axes[1,1].plot(cell_states[:, 1], 'r--', alpha=0.7, label='Cell[1]')\n",
    "    axes[1,1].set_title('Sample Dimensions Evolution')\n",
    "    axes[1,1].set_xlabel('Time Steps')\n",
    "    axes[1,1].set_ylabel('State Value')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize LSTM states for first sequence\n",
    "print(f\"\\nüé® Visualizing LSTM States Evolution:\")\n",
    "visualize_lstm_states(lstm_model, X_seq[0])\n",
    "\n",
    "print(\"\\n‚úÖ LSTM implementation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f4c20",
   "metadata": {},
   "source": [
    "## ‚ö° 5. Simple Gated Recurrent Unit (GRU)\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Simplified gating** compared to LSTM (2 gates vs 3)\n",
    "- **Reset and update gates** control information flow\n",
    "- **No separate cell state** (combined with hidden state)\n",
    "- **Computational efficiency** with similar performance\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "    Reset Gate         Update Gate\n",
    "        ‚Üì                  ‚Üì\n",
    "    œÉ(Wr[h,x]+br)     œÉ(Wu[h,x]+bu)\n",
    "        ‚Üì                  ‚Üì\n",
    "       r_t               z_t\n",
    "        ‚Üì                  ‚Üì\n",
    "   hÃÉ_t = tanh(Wh[r_t*h_{t-1}, x_t] + bh)\n",
    "                    ‚Üì\n",
    "         h_t = (1-z_t)*h_{t-1} + z_t*hÃÉ_t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67230e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Gated Recurrent Unit Implementation\n",
    "\n",
    "class SimpleGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GRU Network for sequence classification.\n",
    "    \n",
    "    Architecture: GRU ‚Üí Linear ‚Üí Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        \n",
    "        # Store architecture info\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,  # Input shape: (batch, seq, feature)\n",
    "            dropout=0.2 if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass through GRU.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, input_size)\n",
    "            hidden: Initial hidden state (optional)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor (batch_size, output_size)\n",
    "            Final hidden state\n",
    "        \"\"\"\n",
    "        # GRU forward pass\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        # Hidden: (num_layers, batch_size, hidden_size)\n",
    "        gru_out, hidden_final = self.gru(x, hidden)\n",
    "        \n",
    "        # Use the last time step output for classification\n",
    "        # Take output from last time step: (batch_size, hidden_size)\n",
    "        last_output = gru_out[:, -1, :]\n",
    "        \n",
    "        # Apply linear layer to get class predictions\n",
    "        output = self.fc(last_output)\n",
    "        \n",
    "        return output, hidden_final\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize hidden state with zeros.\"\"\"\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "    \n",
    "    def get_info(self):\n",
    "        \"\"\"Return model information.\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        return {\n",
    "            'Architecture': 'Gated Recurrent Unit',\n",
    "            'Input Size': self.input_size,\n",
    "            'Hidden Size': self.hidden_size,\n",
    "            'Output Size': self.output_size,\n",
    "            'Num Layers': self.num_layers,\n",
    "            'Total Parameters': total_params\n",
    "        }\n",
    "\n",
    "# Create and test GRU\n",
    "print(\"‚ö° Simple GRU Implementation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Model parameters (same as RNN/LSTM for comparison)\n",
    "input_size = X_seq.shape[2]\n",
    "hidden_size = 32\n",
    "output_size = len(torch.unique(y_seq))\n",
    "\n",
    "# Create model\n",
    "gru_model = SimpleGRU(input_size, hidden_size, output_size, num_layers=1)\n",
    "\n",
    "# Display model info\n",
    "model_info = gru_model.get_info()\n",
    "for key, value in model_info.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä Model Architecture:\")\n",
    "print(gru_model)\n",
    "\n",
    "# Test forward pass\n",
    "sample_input = X_seq[:5]\n",
    "batch_size = sample_input.shape[0]\n",
    "\n",
    "print(f\"\\nüîç Forward Pass Test:\")\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "\n",
    "# Initialize hidden state\n",
    "hidden_init = gru_model.init_hidden(batch_size)\n",
    "print(f\"Initial hidden shape: {hidden_init.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_output, final_hidden = gru_model(sample_input, hidden_init)\n",
    "    \n",
    "print(f\"Output shape: {sample_output.shape}\")\n",
    "print(f\"Final hidden shape: {final_hidden.shape}\")\n",
    "\n",
    "print(f\"Sample outputs (raw logits):\")\n",
    "for i, output in enumerate(sample_output):\n",
    "    predicted_class = torch.argmax(output).item()\n",
    "    actual_class = y_seq[i].item()\n",
    "    confidence = torch.softmax(output, dim=0)[predicted_class].item()\n",
    "    print(f\"  Sample {i+1}: Predicted={predicted_class} (conf: {confidence:.3f}), Actual={actual_class}\")\n",
    "\n",
    "# Compare model complexities\n",
    "print(f\"\\nüìä Model Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "models_comparison = {\n",
    "    'RNN': rnn_model,\n",
    "    'LSTM': lstm_model,\n",
    "    'GRU': gru_model\n",
    "}\n",
    "\n",
    "for name, model in models_comparison.items():\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{name:>6}: {total_params:>6,} parameters\")\n",
    "\n",
    "# Visualize GRU gates behavior (conceptual)\n",
    "def compare_architectures():\n",
    "    \"\"\"Visual comparison of RNN architectures.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # RNN\n",
    "    axes[0].text(0.5, 0.7, 'RNN', ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "    axes[0].text(0.5, 0.5, 'h_t = tanh(W_h[h_{t-1}, x_t] + b_h)', ha='center', va='center', fontsize=10)\n",
    "    axes[0].text(0.5, 0.3, '‚Ä¢ Simple recurrence\\n‚Ä¢ Vanishing gradients\\n‚Ä¢ Fast computation', ha='center', va='center', fontsize=9)\n",
    "    axes[0].set_title('Vanilla RNN')\n",
    "    axes[0].set_xlim(0, 1)\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # LSTM\n",
    "    axes[1].text(0.5, 0.8, 'LSTM', ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "    axes[1].text(0.5, 0.65, '‚Ä¢ Forget Gate: f_t = œÉ(W_f[h,x] + b_f)', ha='center', va='center', fontsize=8)\n",
    "    axes[1].text(0.5, 0.55, '‚Ä¢ Input Gate: i_t = œÉ(W_i[h,x] + b_i)', ha='center', va='center', fontsize=8)\n",
    "    axes[1].text(0.5, 0.45, '‚Ä¢ Output Gate: o_t = œÉ(W_o[h,x] + b_o)', ha='center', va='center', fontsize=8)\n",
    "    axes[1].text(0.5, 0.35, '‚Ä¢ Cell State: C_t = f_t*C_{t-1} + i_t*CÃÉ_t', ha='center', va='center', fontsize=8)\n",
    "    axes[1].text(0.5, 0.2, '‚Ä¢ Long-term memory\\n‚Ä¢ 3 gates + cell state\\n‚Ä¢ More parameters', ha='center', va='center', fontsize=9)\n",
    "    axes[1].set_title('LSTM')\n",
    "    axes[1].set_xlim(0, 1)\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # GRU\n",
    "    axes[2].text(0.5, 0.8, 'GRU', ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "    axes[2].text(0.5, 0.65, '‚Ä¢ Reset Gate: r_t = œÉ(W_r[h,x] + b_r)', ha='center', va='center', fontsize=8)\n",
    "    axes[2].text(0.5, 0.55, '‚Ä¢ Update Gate: z_t = œÉ(W_z[h,x] + b_z)', ha='center', va='center', fontsize=8)\n",
    "    axes[2].text(0.5, 0.45, '‚Ä¢ New State: hÃÉ_t = tanh(W_h[r_t*h,x])', ha='center', va='center', fontsize=8)\n",
    "    axes[2].text(0.5, 0.35, '‚Ä¢ Final: h_t = (1-z_t)*h_{t-1} + z_t*hÃÉ_t', ha='center', va='center', fontsize=8)\n",
    "    axes[2].text(0.5, 0.2, '‚Ä¢ Efficient gating\\n‚Ä¢ 2 gates only\\n‚Ä¢ Fewer parameters', ha='center', va='center', fontsize=9)\n",
    "    axes[2].set_title('GRU')\n",
    "    axes[2].set_xlim(0, 1)\n",
    "    axes[2].set_ylim(0, 1)\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.suptitle('RNN Architecture Comparison', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\nüé® Architecture Comparison:\")\n",
    "compare_architectures()\n",
    "\n",
    "print(\"\\n‚úÖ GRU implementation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b1b023",
   "metadata": {},
   "source": [
    "## üèãÔ∏è 6. Training & Evaluation Functions\n",
    "\n",
    "**Key Training Concepts:**\n",
    "- **Loss functions** (CrossEntropy, MSE)\n",
    "- **Optimizers** (SGD, Adam)\n",
    "- **Learning rate scheduling**\n",
    "- **Gradient clipping** for RNNs\n",
    "- **Validation and metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9013086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation Functions\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"Generic trainer for all neural network models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "    \n",
    "    def train_epoch(self, train_loader, criterion, optimizer, clip_grad=None):\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Handle different model types\n",
    "            if hasattr(self.model, 'init_hidden'):  # RNN-based models\n",
    "                batch_size = data.size(0)\n",
    "                if isinstance(self.model, SimpleLSTM):\n",
    "                    hidden = self.model.init_hidden(batch_size)\n",
    "                    hidden = (hidden[0].to(self.device), hidden[1].to(self.device))\n",
    "                else:\n",
    "                    hidden = self.model.init_hidden(batch_size).to(self.device)\n",
    "                output, _ = self.model(data, hidden)\n",
    "            else:  # ANN/CNN models\n",
    "                output = self.model(data)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for RNN models\n",
    "            if clip_grad:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip_grad)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        \n",
    "        self.train_losses.append(avg_loss)\n",
    "        self.train_accuracies.append(accuracy)\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def validate(self, val_loader, criterion):\n",
    "        \"\"\"Validate the model.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                \n",
    "                # Handle different model types\n",
    "                if hasattr(self.model, 'init_hidden'):  # RNN-based models\n",
    "                    batch_size = data.size(0)\n",
    "                    if isinstance(self.model, SimpleLSTM):\n",
    "                        hidden = self.model.init_hidden(batch_size)\n",
    "                        hidden = (hidden[0].to(self.device), hidden[1].to(self.device))\n",
    "                    else:\n",
    "                        hidden = self.model.init_hidden(batch_size).to(self.device)\n",
    "                    output, _ = self.model(data, hidden)\n",
    "                else:  # ANN/CNN models\n",
    "                    output = self.model(data)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(output, target)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Statistics\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        \n",
    "        self.val_losses.append(avg_loss)\n",
    "        self.val_accuracies.append(accuracy)\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def train(self, train_loader, val_loader, num_epochs=10, lr=0.001, clip_grad=None):\n",
    "        \"\"\"Complete training loop.\"\"\"\n",
    "        # Define loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        print(f\"üöÄ Training {self.model.__class__.__name__} for {num_epochs} epochs...\")\n",
    "        print(f\"üìä Training samples: {len(train_loader.dataset)}\")\n",
    "        print(f\"üìä Validation samples: {len(val_loader.dataset)}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            train_loss, train_acc = self.train_epoch(train_loader, criterion, optimizer, clip_grad)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc = self.validate(val_loader, criterion)\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch+1:2d}/{num_epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                # torch.save(self.model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"\\n‚úÖ Training completed in {training_time:.2f} seconds\")\n",
    "        print(f\"üèÜ Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        \n",
    "        return {\n",
    "            'best_val_acc': best_val_acc,\n",
    "            'final_train_acc': self.train_accuracies[-1],\n",
    "            'final_val_acc': self.val_accuracies[-1],\n",
    "            'training_time': training_time\n",
    "        }\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training and validation metrics.\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        epochs = range(1, len(self.train_losses) + 1)\n",
    "        \n",
    "        # Loss plot\n",
    "        ax1.plot(epochs, self.train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "        ax1.plot(epochs, self.val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        ax2.plot(epochs, self.train_accuracies, 'b-', label='Training Accuracy', linewidth=2)\n",
    "        ax2.plot(epochs, self.val_accuracies, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "        ax2.set_title('Training and Validation Accuracy')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def create_data_loaders(X, y, batch_size=32, train_split=0.8):\n",
    "    \"\"\"Create train and validation data loaders.\"\"\"\n",
    "    # Create dataset\n",
    "    dataset = TensorDataset(X, y)\n",
    "    \n",
    "    # Split dataset\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "print(\"üèãÔ∏è Training and Evaluation Functions Ready!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Demonstrate training setup\n",
    "sample_loader = DataLoader(TensorDataset(X_clf[:100], y_clf[:100]), batch_size=16)\n",
    "sample_trainer = ModelTrainer(ann_model, device)\n",
    "\n",
    "print(f\"‚úÖ Trainer created for {ann_model.__class__.__name__}\")\n",
    "print(f\"üìä Device: {device}\")\n",
    "print(f\"üîß Sample batch size: 16\")\n",
    "print(f\"üíæ Ready for training all models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ef78ca",
   "metadata": {},
   "source": [
    "## üöÄ 7. Test All Models\n",
    "\n",
    "Now let's train and compare all our neural network implementations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test All Neural Network Models\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 5  # Reduced for demo\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"üöÄ Training All Neural Network Models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Test ANN on Classification Data\n",
    "print(\"\\nüß† 1. Testing ANN on Classification Data\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create fresh ANN model\n",
    "ann_model = SimpleANN(X_clf.shape[1], 64, len(torch.unique(y_clf)))\n",
    "ann_train_loader, ann_val_loader = create_data_loaders(X_clf, y_clf, BATCH_SIZE)\n",
    "\n",
    "# Train ANN\n",
    "ann_trainer = ModelTrainer(ann_model, device)\n",
    "ann_results = ann_trainer.train(ann_train_loader, ann_val_loader, EPOCHS, LEARNING_RATE)\n",
    "results['ANN'] = ann_results\n",
    "\n",
    "# Plot training history\n",
    "ann_trainer.plot_training_history()\n",
    "\n",
    "# 2. Test CNN on Image Data\n",
    "print(\"\\nüñºÔ∏è 2. Testing CNN on Image Data\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create fresh CNN model\n",
    "cnn_model = SimpleCNN(X_img.shape[1], len(torch.unique(y_img)))\n",
    "cnn_train_loader, cnn_val_loader = create_data_loaders(X_img, y_img, BATCH_SIZE)\n",
    "\n",
    "# Train CNN\n",
    "cnn_trainer = ModelTrainer(cnn_model, device)\n",
    "cnn_results = cnn_trainer.train(cnn_train_loader, cnn_val_loader, EPOCHS, LEARNING_RATE)\n",
    "results['CNN'] = cnn_results\n",
    "\n",
    "# Plot training history\n",
    "cnn_trainer.plot_training_history()\n",
    "\n",
    "# 3. Test RNN on Sequence Data\n",
    "print(\"\\nüîÑ 3. Testing RNN on Sequence Data\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create fresh RNN model\n",
    "rnn_model = SimpleRNN(X_seq.shape[2], 32, len(torch.unique(y_seq)))\n",
    "rnn_train_loader, rnn_val_loader = create_data_loaders(X_seq, y_seq, BATCH_SIZE)\n",
    "\n",
    "# Train RNN (with gradient clipping)\n",
    "rnn_trainer = ModelTrainer(rnn_model, device)\n",
    "rnn_results = rnn_trainer.train(rnn_train_loader, rnn_val_loader, EPOCHS, LEARNING_RATE, clip_grad=1.0)\n",
    "results['RNN'] = rnn_results\n",
    "\n",
    "# Plot training history\n",
    "rnn_trainer.plot_training_history()\n",
    "\n",
    "# 4. Test LSTM on Sequence Data\n",
    "print(\"\\nüßÆ 4. Testing LSTM on Sequence Data\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create fresh LSTM model\n",
    "lstm_model = SimpleLSTM(X_seq.shape[2], 32, len(torch.unique(y_seq)))\n",
    "lstm_train_loader, lstm_val_loader = create_data_loaders(X_seq, y_seq, BATCH_SIZE)\n",
    "\n",
    "# Train LSTM\n",
    "lstm_trainer = ModelTrainer(lstm_model, device)\n",
    "lstm_results = lstm_trainer.train(lstm_train_loader, lstm_val_loader, EPOCHS, LEARNING_RATE, clip_grad=1.0)\n",
    "results['LSTM'] = lstm_results\n",
    "\n",
    "# Plot training history\n",
    "lstm_trainer.plot_training_history()\n",
    "\n",
    "# 5. Test GRU on Sequence Data\n",
    "print(\"\\n‚ö° 5. Testing GRU on Sequence Data\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create fresh GRU model\n",
    "gru_model = SimpleGRU(X_seq.shape[2], 32, len(torch.unique(y_seq)))\n",
    "gru_train_loader, gru_val_loader = create_data_loaders(X_seq, y_seq, BATCH_SIZE)\n",
    "\n",
    "# Train GRU\n",
    "gru_trainer = ModelTrainer(gru_model, device)\n",
    "gru_results = gru_trainer.train(gru_train_loader, gru_val_loader, EPOCHS, LEARNING_RATE, clip_grad=1.0)\n",
    "results['GRU'] = gru_results\n",
    "\n",
    "# Plot training history\n",
    "gru_trainer.plot_training_history()\n",
    "\n",
    "# Final Results Comparison\n",
    "print(\"\\nüìä FINAL RESULTS COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = []\n",
    "for model_name, result in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Best Val Acc (%)': f\"{result['best_val_acc']:.2f}\",\n",
    "        'Final Train Acc (%)': f\"{result['final_train_acc']:.2f}\",\n",
    "        'Final Val Acc (%)': f\"{result['final_val_acc']:.2f}\",\n",
    "        'Training Time (s)': f\"{result['training_time']:.2f}\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Model parameter comparison\n",
    "print(f\"\\nüîß MODEL COMPLEXITY COMPARISON\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "models_for_comparison = [\n",
    "    ('ANN', ann_model),\n",
    "    ('CNN', cnn_model), \n",
    "    ('RNN', rnn_model),\n",
    "    ('LSTM', lstm_model),\n",
    "    ('GRU', gru_model)\n",
    "]\n",
    "\n",
    "for name, model in models_for_comparison:\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"{name:>6}: {total_params:>8,} total params ({trainable_params:>8,} trainable)\")\n",
    "\n",
    "# Performance visualization\n",
    "def plot_final_comparison():\n",
    "    \"\"\"Plot final performance comparison.\"\"\"\n",
    "    model_names = list(results.keys())\n",
    "    val_accuracies = [results[name]['best_val_acc'] for name in model_names]\n",
    "    training_times = [results[name]['training_time'] for name in model_names]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    bars1 = ax1.bar(model_names, val_accuracies, alpha=0.8, color=['blue', 'green', 'red', 'orange', 'purple'])\n",
    "    ax1.set_title('Best Validation Accuracy Comparison')\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.set_ylim(0, 100)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars1, val_accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Training time comparison\n",
    "    bars2 = ax2.bar(model_names, training_times, alpha=0.8, color=['blue', 'green', 'red', 'orange', 'purple'])\n",
    "    ax2.set_title('Training Time Comparison')\n",
    "    ax2.set_ylabel('Time (seconds)')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, time_val in zip(bars2, training_times):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{time_val:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\nüé® Performance Visualization:\")\n",
    "plot_final_comparison()\n",
    "\n",
    "# Key takeaways\n",
    "print(f\"\\nüìö KEY TAKEAWAYS FOR EXAM\")\n",
    "print(\"=\" * 50)\n",
    "print(\"üß† ANN: Simple feedforward, good for tabular data\")\n",
    "print(\"üñºÔ∏è CNN: Excellent for images, spatial feature extraction\")\n",
    "print(\"üîÑ RNN: Sequential data, but prone to vanishing gradients\") \n",
    "print(\"üßÆ LSTM: Best for long sequences, handles long-term dependencies\")\n",
    "print(\"‚ö° GRU: Efficient alternative to LSTM, fewer parameters\")\n",
    "print(\"\\nüéØ Model Selection Guidelines:\")\n",
    "print(\"‚Ä¢ Tabular/structured data ‚Üí ANN\")\n",
    "print(\"‚Ä¢ Images/spatial data ‚Üí CNN\") \n",
    "print(\"‚Ä¢ Short sequences ‚Üí RNN\")\n",
    "print(\"‚Ä¢ Long sequences ‚Üí LSTM/GRU\")\n",
    "print(\"‚Ä¢ Resource constrained ‚Üí GRU over LSTM\")\n",
    "\n",
    "print(\"\\n‚úÖ ALL NEURAL NETWORK IMPLEMENTATIONS COMPLETE!\")\n",
    "print(\"üéì Ready for your exam! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c854fad",
   "metadata": {},
   "source": [
    "## üìö Exam Cheat Sheet - Neural Network Quick Reference\n",
    "\n",
    "### üß† **ANN (Artificial Neural Network)**\n",
    "```python\n",
    "# Key Components:\n",
    "- Linear layers: nn.Linear(input_size, output_size)\n",
    "- Activation: ReLU, Sigmoid, Tanh\n",
    "- Loss: CrossEntropyLoss, MSELoss\n",
    "- Optimizer: Adam, SGD\n",
    "\n",
    "# When to use: Tabular data, classification/regression\n",
    "# Pros: Simple, fast training\n",
    "# Cons: No spatial/temporal awareness\n",
    "```\n",
    "\n",
    "### üñºÔ∏è **CNN (Convolutional Neural Network)**\n",
    "```python\n",
    "# Key Components:\n",
    "- Conv2d: nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "- Pooling: nn.MaxPool2d(kernel_size)\n",
    "- Flatten: x.view(x.size(0), -1)\n",
    "\n",
    "# When to use: Images, spatial data\n",
    "# Pros: Translation invariant, parameter sharing\n",
    "# Cons: Limited to grid-like data\n",
    "```\n",
    "\n",
    "### üîÑ **RNN (Recurrent Neural Network)**\n",
    "```python\n",
    "# Key Components:\n",
    "- Hidden state: h_t = tanh(W_ih * x_t + W_hh * h_{t-1} + b)\n",
    "- Sequential processing\n",
    "- Vanishing gradient problem\n",
    "\n",
    "# When to use: Short sequences, simple temporal patterns\n",
    "# Pros: Memory of previous inputs\n",
    "# Cons: Vanishing gradients, slow training\n",
    "```\n",
    "\n",
    "### üßÆ **LSTM (Long Short-Term Memory)**\n",
    "```python\n",
    "# Key Components:\n",
    "- Forget gate: f_t = œÉ(W_f * [h_{t-1}, x_t] + b_f)\n",
    "- Input gate: i_t = œÉ(W_i * [h_{t-1}, x_t] + b_i)\n",
    "- Output gate: o_t = œÉ(W_o * [h_{t-1}, x_t] + b_o)\n",
    "- Cell state: C_t = f_t * C_{t-1} + i_t * tanh(W_C * [h_{t-1}, x_t] + b_C)\n",
    "\n",
    "# When to use: Long sequences, language modeling\n",
    "# Pros: Handles long-term dependencies, no vanishing gradients\n",
    "# Cons: More parameters, slower than GRU\n",
    "```\n",
    "\n",
    "### ‚ö° **GRU (Gated Recurrent Unit)**\n",
    "```python\n",
    "# Key Components:\n",
    "- Reset gate: r_t = œÉ(W_r * [h_{t-1}, x_t])\n",
    "- Update gate: z_t = œÉ(W_z * [h_{t-1}, x_t])\n",
    "- New memory: √±_t = tanh(W * [r_t * h_{t-1}, x_t])\n",
    "- Hidden state: h_t = (1 - z_t) * h_{t-1} + z_t * √±_t\n",
    "\n",
    "# When to use: Long sequences, resource constraints\n",
    "# Pros: Fewer parameters than LSTM, faster training\n",
    "# Cons: May not capture very long dependencies as well as LSTM\n",
    "```\n",
    "\n",
    "### üéØ **Quick Decision Tree for Model Selection**\n",
    "```\n",
    "Data Type?\n",
    "‚îú‚îÄ‚îÄ Tabular/Structured ‚Üí ANN\n",
    "‚îú‚îÄ‚îÄ Images/Spatial ‚Üí CNN\n",
    "‚îî‚îÄ‚îÄ Sequential\n",
    "    ‚îú‚îÄ‚îÄ Short sequences ‚Üí RNN\n",
    "    ‚îú‚îÄ‚îÄ Long sequences + accuracy critical ‚Üí LSTM\n",
    "    ‚îî‚îÄ‚îÄ Long sequences + speed critical ‚Üí GRU\n",
    "```\n",
    "\n",
    "### üìä **Common Hyperparameters**\n",
    "- **Learning Rate**: 0.001 (Adam), 0.01 (SGD)\n",
    "- **Batch Size**: 32, 64, 128\n",
    "- **Hidden Layers**: 1-3 for simple tasks\n",
    "- **Hidden Units**: 64, 128, 256\n",
    "- **Dropout**: 0.2-0.5 for regularization\n",
    "- **Epochs**: 10-100 depending on data size\n",
    "\n",
    "### üõ†Ô∏è **PyTorch Essentials**\n",
    "```python\n",
    "# Basic training loop\n",
    "for epoch in range(epochs):\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Key methods\n",
    "model.train()    # Training mode\n",
    "model.eval()     # Evaluation mode\n",
    "torch.no_grad()  # Disable gradients for inference\n",
    "```\n",
    "\n",
    "### üéì **Exam Tips**\n",
    "1. **Understand the mathematical formulas** behind each architecture\n",
    "2. **Know when to use each model type** based on data characteristics\n",
    "3. **Remember the vanishing gradient problem** and how LSTM/GRU solve it\n",
    "4. **Practice implementing** basic versions from scratch\n",
    "5. **Understand the role of gates** in LSTM/GRU\n",
    "6. **Know common hyperparameters** and their typical ranges\n",
    "\n",
    "---\n",
    "**üí° Good luck with your exam! This notebook covers all the essential neural network implementations you need to know.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
